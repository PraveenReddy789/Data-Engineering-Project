# Data-Engineering-Project

Developed a Comprehensive ETL Pipeline: Created dbt models for transforming and analyzing data, including combining and preparing data from multiple sources, and performing advanced data analysis using SQL.

Implemented Airflow DAGs for Orchestration: Set up Apache Airflow environment with dbt integration, designing and deploying DAGs to manage data transformations and analysis workflows efficiently.

Extended Functionality with Snowpark: Incorporated Snowpark into the Airflow environment to perform Python-based data analysis, leveraging Snowpark for enhanced data manipulation and computation.

Configured Docker and Virtual Environments: Customized Dockerfile to include Python 3.8 for Snowpark and managed dependencies using virtual environments, ensuring compatibility and smooth execution of data tasks.

Built and Deployed Streamlit Dashboard: Set up a Streamlit application for interactive data visualization, allowing real-time data exploration and insights through a user-friendly web interface.

Optimized Data Integration and Analysis: Integrated Snowflake with dbt and Airflow to streamline data processing, achieving efficient data workflows and insightful analysis.
